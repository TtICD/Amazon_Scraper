name: Daily Scraper

on:
  schedule:
    - cron: "0 0 * * *" 
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3

    - name: Install Python and Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3 python3-pip
        python3 -m pip install selenium==4.27.1 
        python3 -m pip install pandas==2.0.3

    - name: Install Chrome 
      run: |      
        sudo apt-get update
        sudo apt-get install -y wget curl
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo dpkg -i google-chrome-stable_current_amd64.deb
        sudo apt-get install -f  


    - name: Install ChromeDriver
      run: |       
        CHROME_VERSION=$(google-chrome --version | grep -oE '[0-9.]+' | head -1)
        DRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
        
        echo "Using ChromeDriver version: $DRIVER_VERSION"
        
        # Download and install ChromeDriver
        wget -q "https://chromedriver.storage.googleapis.com/$DRIVER_VERSION/chromedriver_linux64.zip"
        unzip chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/
        
        wget -q "https://chromedriver.storage.googleapis.com/$DRIVER_VERSION/chromedriver_linux64.zip"
        unzip chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/


    - name: Run Scraper
      run: python3 daily_scraper.py

    - name: Commit and Push CSV Changes
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add data/cumulative_data.csv  # Add your updated CSV file
        git commit -m "Update CSV with new scraped data"
        git push origin main
